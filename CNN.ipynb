{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d6f5d5-7f54-4e1d-a9c0-fc1b78f9cd48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47dd5108-990c-4fc5-8df0-1b92b0831180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms \n",
    "from torch.utils.data import random_split, DataLoader\n",
    "# Check cuda availbility \n",
    "print('CUDA is available!') if  torch.cuda.is_available() else print('No cuda')\n",
    "# assign cuda to device\n",
    "device = torch.device('cuda')\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9a5fe-7f10-44d5-86c0-cea5a7e14e26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example 2: CNN for CIFAR10\n",
    "5 main components \n",
    "- model \n",
    "- load data \n",
    "- trainig loop \n",
    "- validation loop \n",
    "- hparams, optmizer, and loss\n",
    "- Start the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea6d14-678d-4b2d-b6f7-9d35531aa8a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1- Build model Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c423dbdd-ef62-4e28-93af-6d65c63b648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model definition for CIFCAR10 datset       \n",
    "class CNN_M1(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super().__init__()\n",
    "        # convolutional layer (sees 32x32x3 image tensor if CFAR)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, 3, padding=1)\n",
    "        # convolutional layer (sees 16x16x16 tensor)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        # convolutional layer (sees 8x8x32 tensor)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (64 * 4 * 4 -> 500)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n",
    "        # linear layer (500 -> 10)\n",
    "        self.fc2 = nn.Linear(500, output_size)\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CNN_M2(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            # convolutional layer (sees 32x32x3 image tensor if CFAR)\n",
    "            nn.Conv2d(input_channels, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # convolutional layer (sees 16x16x16 tensor)\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # convolutional layer (sees 8x8x32 tensor)\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # flatenning\n",
    "            nn.Flatten(),\n",
    "            # linear layer (64 * 4 * 4 -> 500)\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64 * 4 * 4, 500),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # linear layer (500 -> 10)\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(500, output_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        scores = self.cnn(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c49d7-306c-4af1-a4b5-2407755d4395",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2- Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f91011-48bd-44a2-b53f-4854b9b40144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CIFAR10():\n",
    "    # Data transforms and augmentation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(), # convert PIL image to Tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) # normalizing each channel mean and std\n",
    "\n",
    "    # Load CIFAR10 train data\n",
    "    train_data = datasets.CIFAR10('data', train=True, download=True, transform=transform)\n",
    "  # load test data \n",
    "    test_data = datasets.CIFAR10('data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split train data to train and val\n",
    "    val_size = 0.2 \n",
    "    num_val_samples = int(0.2 * len(train_data))\n",
    "    num_train_samples = len(train_data) - num_val_samples\n",
    "    train, val = random_split(train_data, [num_train_samples, num_val_samples])\n",
    "\n",
    "  \n",
    "    # Pass the data to the dataloader\n",
    "    train_loader = DataLoader(train, batch_size = 32)\n",
    "    val_loader = DataLoader(val, batch_size= 32)\n",
    "    test_loader = DataLoader(test_data, batch_size= 32)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d3fe922-cea8-4178-a9b5-f70118979ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# loading mnist data\n",
    "train_dl, val_dl, test_dl = load_CIFAR10()\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce9c13e-bf84-4b5f-9154-bb4eed161ebe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3- Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dcce449-16f8-484a-af43-fd4c466194e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(epochs, optimizer, criterion, model):\n",
    "    # loop through each epoch\n",
    "    for epoch in range(epochs):\n",
    "        # losses and accuraces accumulate\n",
    "        losses =[]\n",
    "        accuracies =[]\n",
    "        \n",
    "        # activate model train\n",
    "        model.train()\n",
    "        \n",
    "        # loop through each batch\n",
    "        for batch_idx, (data, labels) in enumerate(train_dl):\n",
    "            # send data and taret to cuda\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            scores = model(data)\n",
    "\n",
    "            # compute the loss function \n",
    "            loss = criterion(scores, labels)\n",
    "\n",
    "            # clear the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step based on the computed gradients\n",
    "            optimizer.step()\n",
    "            # append loss for each batch\n",
    "            losses.append(loss.detach().item()) # deteach the loss from compute graph \n",
    "            accuracies.append(labels.eq(scores.detach().argmax(dim=1)).float().mean())\n",
    "        \n",
    "        # print for each epoch\n",
    "        print(f'Epoch: {epoch + 1} \\n \\t Train Loss:{torch.tensor(losses).mean():0.2f} \\t Train Acc:{torch.tensor(accuracies).mean():0.2f}')\n",
    "\n",
    "        # for each epoch evaluate your model on the validation data\n",
    "        val_loss, val_acc = model_evaluate(criterion, model, phase = 'Val')\n",
    "        print(f'\\t Val Loss:{val_loss:0.2f} \\t\\t Val Acc:{val_acc:0.2f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97690ef3-8e65-4cd6-a1ce-cb417d16b699",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4- Val loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19be3cd7-cb07-43e4-a438-cacbaffe2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(criterion, model, phase = 'Val'): # phase can be validate or test\n",
    "    # append losses and accuracies\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    # activate model eval function\n",
    "    model.eval()\n",
    "    \n",
    "    dataloader = val_dl if phase == 'Val' else test_dl\n",
    "    # loop through val data loader\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "            # send data and taret to cuda\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            scores = model(data)\n",
    "\n",
    "            # compute the loss function \n",
    "            loss = criterion(scores, labels)\n",
    "\n",
    "            # append loss for each batch\n",
    "            losses.append(loss.detach().item()) # deteach the loss from compute graph \n",
    "            accuracies.append(labels.eq(scores.detach().argmax(dim=1)).float().mean())\n",
    "        mean_loss = torch.tensor(losses).mean()\n",
    "        mean_acc =torch.tensor(accuracies).mean()\n",
    "        return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761fdf4-2eb3-4d7e-bb9a-a04ea1e7798e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5- Optimizer and hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "130fd5fa-17e7-4d05-a4e5-b6710671df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contruct model object\n",
    "num_channels = 3\n",
    "num_classes = 10\n",
    "my_cnn = CNN_M2(num_channels,num_classes).to(device)\n",
    "# Define my optimiser\n",
    "params = my_cnn.parameters()\n",
    "# Using adam optimizer\n",
    "optimizer = optim.SGD(params, lr=0.01, momentum=0.9)\n",
    "\n",
    "# Define my loss\n",
    "# here we use cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# hparams \n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775eff6-2e95-458e-8d15-108b8b383bfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Now lets start training CNN !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94b410c1-6994-4187-b493-4b6588267c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      " \t Train Loss:1.73 \t Train Acc:0.36\n",
      "\t Val Loss:1.38 \t\t Val Acc:0.49\n",
      "Epoch: 2 \n",
      " \t Train Loss:1.33 \t Train Acc:0.52\n",
      "\t Val Loss:1.15 \t\t Val Acc:0.59\n",
      "Epoch: 3 \n",
      " \t Train Loss:1.17 \t Train Acc:0.58\n",
      "\t Val Loss:1.05 \t\t Val Acc:0.62\n",
      "Epoch: 4 \n",
      " \t Train Loss:1.08 \t Train Acc:0.62\n",
      "\t Val Loss:0.97 \t\t Val Acc:0.66\n",
      "Epoch: 5 \n",
      " \t Train Loss:1.02 \t Train Acc:0.64\n",
      "\t Val Loss:0.95 \t\t Val Acc:0.67\n"
     ]
    }
   ],
   "source": [
    "cnn_trained = model_train(num_epochs, optimizer, criterion, my_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea50363-d4f9-4530-a0f5-061f2553d384",
   "metadata": {},
   "source": [
    "#### Testing onthe test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1880956d-13cb-4b31-903a-0a85a61c1fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:0.95\n",
      "Test Acc:0.66\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_evaluate(criterion, cnn_trained, phase='test')\n",
    "print(f'Test Loss:{test_loss.item():0.2f}')\n",
    "print(f'Test Acc:{test_acc.item():0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d41d53-5b71-4037-bc56-0b6bcd7dc8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
