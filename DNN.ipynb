{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67aa688f-dd65-46a3-9573-c6c91d077cfb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37b3e9b-3ed8-44b3-9eb0-5ea9bc1bdf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms \n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Check cuda availbility \n",
    "print('CUDA is available!') if  torch.cuda.is_available() else print('No cuda')\n",
    "# assign cuda to device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device= torch.device('cpu')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef76d8-3ef0-4038-ae59-9bfcd0e55d4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example 1: DNN for MNIST\n",
    "5 main components \n",
    "- model \n",
    "- load data \n",
    "- trainig loop \n",
    "- validation loop \n",
    "- hparams, optmizer, and loss\n",
    "- Start the training process\n",
    "- Test on the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f87ec-d6a1-4698-8ab9-16d52afa58f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1- Build Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d3f0ae7-e2c3-461e-b093-242e45004370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN model \n",
    "# class DNN_conventional(nn.Module):\n",
    "#     def __init__(self, input_size, output_size): # contructor method for the subclass \n",
    "#         super().__init__() # super allows you to call methods of the superclass in your subclass \n",
    "#         # building the DNN model using sequential \n",
    "#         # default way\n",
    "#         self.l1 = nn.Linear(input_size, 64)\n",
    "#         self.l2 = nn.Linear(64,64)\n",
    "#         self.l3 = nn.Linear(64,output_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = self.l1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.l2(out)\n",
    "\n",
    "#         return out\n",
    "            \n",
    " \n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size): # contructor method for the subclass \n",
    "        super().__init__() # super allows you to call methods of the superclass in your subclass \n",
    "        # sequential way\n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "                )\n",
    "\n",
    "    # def forward function for the model\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        # x: batch_size, width, length\n",
    "        # vectorize the input\n",
    "        x = x.view(batch_size, -1)\n",
    "        scores = self.dnn(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddc53b9c-768c-4aae-8eda-0e739ac54a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (dnn): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define input and output size \n",
    "# given that the input shape for mnist is: 28 x 28, the vector length = 28^2\n",
    "input_size = 28**2\n",
    "\n",
    "# output size = num of classes = 10 \n",
    "output_size = 10\n",
    "\n",
    "# create object from the class \n",
    "my_dnn = DNN(input_size, output_size).to(device)\n",
    "my_dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9f5d3-aff6-4961-81a9-2b32b764fa6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2- Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1babb0b8-6406-4a3d-a4d2-f76986303206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST():\n",
    "    # Data transforms \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # convert PIL image to Tensor\n",
    "        transforms.Normalize((0.5), (1)) ]) # normalizing each channel with 0.5  mean and std\n",
    "    # Load load_MNIST train data\n",
    "    train_data = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "\n",
    "    # Split train data to train and val\n",
    "    val_size = 0.2 \n",
    "    num_val_samples = int(0.2 * len(train_data))\n",
    "    num_train_samples = len(train_data) - num_val_samples\n",
    "    train, val = random_split(train_data, [num_train_samples, num_val_samples])\n",
    "\n",
    "    # load test data \n",
    "    test_data = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    \n",
    "    # Pass the data to the dataloader\n",
    "    train_loader = DataLoader(train, batch_size = 32)\n",
    "    val_loader = DataLoader(val, batch_size= 32)\n",
    "    test_loader = DataLoader(test_data, batch_size= 32)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "# loading mnist data\n",
    "train_dl, val_dl, test_dl = load_MNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5305db-9d3a-46cb-83cb-eaee3e009995",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3- Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75343daf-df78-4672-a6a6-535b4c2e398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(epochs, optimizer, criterion, model):\n",
    "    # loop through each epoch\n",
    "    for epoch in range(epochs):\n",
    "        # losses and accuraces accumulate\n",
    "        losses =[]\n",
    "        accuracies =[]\n",
    "        \n",
    "        # activate model train\n",
    "        my_dnn.train()\n",
    "        \n",
    "        # loop through each batch\n",
    "        # forward pass\n",
    "        # calculate loss\n",
    "        # based on loss calculate gradients with respect model params\n",
    "        # I ask optimizer update the model parameters given\n",
    "        for batch_idx, (data, labels) in enumerate(train_dl):\n",
    "            # send data and target to cuda\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            scores = model(data)\n",
    "\n",
    "            # compute the loss function \n",
    "            loss = criterion(scores, labels)\n",
    "\n",
    "            # clear the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # perform a single optimization step based on the computed gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # append loss for each batch\n",
    "            losses.append(loss.detach().item()) # deteach the loss from compute graph \n",
    "            accuracies.append(labels.eq(scores.detach().argmax(dim=1)).float().mean())\n",
    "        \n",
    "        # print for each epoch\n",
    "        print(f'Epoch: {epoch + 1} \\n \\t Train Loss:{torch.tensor(losses).mean():0.2f} \\t Train Acc:{torch.tensor(accuracies).mean():0.2f}')\n",
    "\n",
    "        # for each epoch evaluate your model on the validation data\n",
    "        val_loss, val_acc = model_evaluate(criterion, model, phase = 'Val')\n",
    "        print(f'\\t Val Loss:{val_loss:0.2f} \\t\\t Val Acc:{val_acc:0.2f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cdce7-a2f3-4837-9774-7a8b2544679b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 4- Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4417c1c-2772-4c61-9e2e-1e284497bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(criterion, model, phase = 'Val'): # phase can be validate or test\n",
    "    # append losses and accuracies\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    # activate model eval function\n",
    "    my_dnn.eval()\n",
    "    \n",
    "    dataloader = val_dl if phase == 'Val' else test_dl\n",
    "    # loop through val data loader\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "            # send data and taret to cuda\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            scores = model(data)\n",
    "\n",
    "            # compute the loss function \n",
    "            loss = criterion(scores, labels)\n",
    "\n",
    "            # append loss for each batch\n",
    "            losses.append(loss.detach().item()) # deteach the loss from compute graph \n",
    "            accuracies.append(labels.eq(scores.detach().argmax(dim=1)).float().mean())\n",
    "            \n",
    "        mean_loss = torch.tensor(losses).mean()\n",
    "        mean_acc =torch.tensor(accuracies).mean()\n",
    "        \n",
    "        return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6557bc4-8d94-450b-a88e-6f1700ef60ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 5- Hparams and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "974540ed-a7e6-41c7-8aac-796acc9e362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my optimiser\n",
    "params = my_dnn.parameters()\n",
    "# Using adam optimizer\n",
    "optimizer = optim.AdamW(params, lr=1e-2)\n",
    "\n",
    "# Define my loss\n",
    "# here we use cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# hparams \n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15452b44-559d-41c5-9128-1498e00e27c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 6- Now Lets Start the train and test ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a5410a0-c6fe-4b39-968a-f36a41587fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      " \t Train Loss:0.41 \t Train Acc:0.87\n",
      "\t Val Loss:0.28 \t\t Val Acc:0.92\n",
      "Epoch: 2 \n",
      " \t Train Loss:0.29 \t Train Acc:0.91\n",
      "\t Val Loss:0.29 \t\t Val Acc:0.91\n",
      "Epoch: 3 \n",
      " \t Train Loss:0.27 \t Train Acc:0.92\n",
      "\t Val Loss:0.25 \t\t Val Acc:0.93\n",
      "Epoch: 4 \n",
      " \t Train Loss:0.27 \t Train Acc:0.92\n",
      "\t Val Loss:0.32 \t\t Val Acc:0.91\n",
      "Epoch: 5 \n",
      " \t Train Loss:0.25 \t Train Acc:0.93\n",
      "\t Val Loss:0.29 \t\t Val Acc:0.92\n"
     ]
    }
   ],
   "source": [
    "# model training \n",
    "train_model = model_train(num_epochs, optimizer,criterion, my_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3fbd73f-7bc5-46ca-98cd-25acdd2271b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:0.29\n",
      "Test Acc:0.92\n"
     ]
    }
   ],
   "source": [
    "# testing the model on test data \n",
    "test_loss, test_acc = model_evaluate(criterion, train_model, phase='test')\n",
    "print(f'Test Loss:{test_loss.item():0.2f}')\n",
    "print(f'Test Acc:{test_acc.item():0.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
